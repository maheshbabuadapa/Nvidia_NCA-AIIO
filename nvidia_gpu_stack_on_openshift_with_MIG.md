# ğŸ§  NVIDIA GPU Stack Integration on OpenShift (Kubernetes) â€” with MIG

## 1ï¸âƒ£ Overview

This guide explains how **NVIDIAâ€™s AI inference stack** â€” including **NGC**, **Dynamo**, **Triton**, **CUDA**, and **MIG** â€” fits together inside a **Kubernetes or OpenShift cluster** to run GPU-accelerated workloads such as Large Language Models (LLMs).

---

## 2ï¸âƒ£ Key Components

### ğŸ—‚ï¸ NVIDIA NGC (NVIDIA GPU Cloud)
- NVIDIAâ€™s official container registry and model hub.  
- Provides pre-built, optimized Docker images, pretrained models, and deployment resources.  
- Examples: `nvcr.io/nvidia/tritonserver:<tag>`, `nvcr.io/nvidia/dynamo:<tag>`.  
- Analogy: *Docker Hub + Hugging Face + App Store* for NVIDIA software.

### âš™ï¸ NVIDIA DYNAMO
- Distributed inference orchestrator for large LLMs.  
- Manages how workloads are split and scheduled across many GPU nodes.  
- Key features: multi-GPU scheduling, prefill/decode separation, load balancing.  
- Analogy: **Kubernetes control plane** for AI inference.

### ğŸ§© NVIDIA TRITON INFERENCE SERVER
- Model-serving runtime running inside each GPU node.  
- Executes inference on models using TensorRT-LLM or PyTorch.  
- Capabilities: multi-framework support, batching, HTTP/gRPC inference APIs.  
- Analogy: **kubelet + Docker Engine** for GPU workloads.

### âš¡ CUDA / TensorRT / Drivers
- Low-level GPU runtime stack communicating with hardware.  
- Components: CUDA, cuDNN, NCCL, TensorRT.  
- Installed via NVIDIA GPU Operator in OpenShift.

---

## 3ï¸âƒ£ Integration Layer Diagram

```
App (Chatbot / RAG / API)
          â”‚
          â–¼
NVIDIA DYNAMO (Orchestrator)
          â”‚
          â–¼
NVIDIA TRITON (Model Server)
          â”‚
          â–¼
CUDA / TensorRT / Drivers
          â”‚
          â–¼
GPU Hardware (A100 / H100)
          â–²
          â”‚
NVIDIA NGC Registry (source for containers & models)
```

---

## 4ï¸âƒ£ OpenShift Integration Flow

1. Install **NVIDIA GPU Operator** (adds driver, CUDA, device plugin).  
2. Pull NVIDIA containers from **NGC**.  
3. Deploy **Triton** pods on GPU nodes.  
4. Deploy **Dynamo** as orchestrator.  
5. Expose Dynamo via OpenShift Route.  
6. Application â†’ Dynamo â†’ Triton â†’ CUDA â†’ GPU.

---

## 5ï¸âƒ£ Analogy Table

| Component | Role | Analogy |
|------------|------|---------|
| NGC | Distribution for containers/models | Docker Hub |
| Dynamo | Orchestrator | Kubernetes master |
| Triton | Local executor | kubelet + Docker |
| CUDA / TensorRT | Compute runtime | CPU instruction set |
| GPU Operator | Device setup | Cloud-init for GPUs |

---

## 6ï¸âƒ£ Simplified Cluster Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ OpenShift / Kubernetes Cluster                              â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ NVIDIA DYNAMO (acts like K8s Scheduler)                â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚    â”‚                         â”‚                               â”‚
â”‚    â–¼                         â–¼                               â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚ â”‚ TRITON Node1 â”‚     â”‚ TRITON Node2 â”‚                        â”‚
â”‚ â”‚ CUDA/TensorRTâ”‚     â”‚ CUDA/TensorRTâ”‚                        â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚        â”‚ Drivers & GPU Operator                               â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚ â”‚ Physical GPU A100  â”‚ â”‚ Physical GPU H100   â”‚               â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚ [Containers pulled from NGC]                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 7ï¸âƒ£ Summary Checklist

| Step | Task | Tool |
|------|------|------|
| 1 | Install GPU Operator | OpenShift OperatorHub |
| 2 | Pull Containers | NGC (nvcr.io) |
| 3 | Deploy Triton Pods | OpenShift Deployment |
| 4 | Deploy Dynamo | StatefulSet |
| 5 | Expose Services | Route/Ingress |
| 6 | Send Requests | App â†’ Dynamo â†’ Triton â†’ CUDA â†’ GPU |

---

## 8ï¸âƒ£ Multiâ€‘Instance GPU (MIG)

### ğŸ”¹ What is MIG
**MIG (Multiâ€‘Instance GPU)** is available on NVIDIA A100, H100, and newer GPUs.  
It splits one physical GPU into multiple smaller, isolated â€œvirtual GPUs,â€ each with dedicated compute and memory.

Each instance behaves as an independent GPU with guaranteed resources.

### âš™ï¸ How MIG Works

| MIG Profile | Meaning | Memory | Instances per GPU | Use Case |
|--------------|----------|---------|-------------------|----------|
| 1g.5gb | 1 GPU slice + 5â€¯GB memory | 5â€¯GB | up toâ€¯7 | Small inference jobs |
| 2g.10gb | 2 GPU slices + 10â€¯GB memory | 10â€¯GB | up toâ€¯3 | Medium workloads |
| 3g.20gb | 3 GPU slices + 20â€¯GB memory | 20â€¯GB | up toâ€¯2 | Large models |
| 7g.40gb | Full GPU | 40â€¯GB | 1 | Full LLM training/inference |

ğŸ§© **Format:** `<g>g.<m>gb` â†’ â€œgâ€ = compute portion, â€œgbâ€ = memory size.

**Example:** `1g.5gb` â†’ one-seventh of the GPU with 5â€¯GB HBM memory.

### ğŸ• Pizza Analogy
Imagine your GPU as a pizza:  
- `1g.5gb` â†’ 7 small slices  
- `2g.10gb` â†’ 3 medium slices  
- `7g.40gb` â†’ whole pizza.  
Each slice is separate â€” no one eats anotherâ€™s share!

### ğŸ–¥ï¸ MIG in OpenShift Stack

```
App â†’ Dynamo â†’ Triton â†’ CUDA â†’ MIG Instances â†’ Physical GPU
```

Each MIG instance appears to CUDA and Triton as a separate GPU device.

### ğŸ§© Steps to Enable MIG

```bash
sudo nvidia-smi -i 0 -mig 1                 # Enable MIG mode
sudo nvidia-smi mig -cgi 1g.5gb,2g.10gb -C  # Create instances
```

Kubernetes (via the GPU Operator) detects each MIG slice as:
```
nvidia.com/mig-1g.5gb
nvidia.com/mig-2g.10gb
```

Pods can request MIG resources:
```yaml
resources:
  limits:
    nvidia.com/mig-1g.5gb: 1
```

### ğŸš€ Benefits of MIG

| Benefit | Description |
|----------|-------------|
| Better GPU utilization | Share one GPU across workloads |
| Isolation | Each instance has its own memory and compute |
| Predictable performance | Workloads donâ€™t interfere |
| Cost efficiency | Multi-tenant friendly |
| Scalability | Many small pods on one GPU |

### âœ… Summary
> **MIG = GPU Virtualization.**  
> It divides a GPU into isolated slices, each seen as a full GPU by CUDA, Triton, and Dynamo.  
> In the stack, MIG sits **between CUDA and the physical GPU**, giving OpenShift fine-grained GPU scheduling.
